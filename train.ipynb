{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:20:39.086880800Z",
     "start_time": "2025-01-06T00:20:39.074679900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# scikit-learn scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def month_range(start, end):\n",
    "    \"\"\"\n",
    "    Return a list of monthly dates (as strings 'YYYY-MM-01')\n",
    "    from start to end inclusive.\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    cur = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    stop = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    while cur <= stop:\n",
    "        dates.append(cur.strftime(\"%Y-%m-01\"))\n",
    "        # move forward one month\n",
    "        cur += relativedelta(months=1)\n",
    "    return dates\n",
    "\n",
    "def drop_all_nan_columns_before_cutoff(arr, cutoff_idx):\n",
    "    \"\"\"\n",
    "    Drop the columns that are all NaNs before or at the cutoff index.\n",
    "    :param arr: np.array of shape (n, m)\n",
    "    :return: np.array of shape (n, m') where m' <= m\n",
    "    \"\"\"\n",
    "    mask = np.all(np.isnan(arr[:cutoff_idx, :]), axis=0)\n",
    "\n",
    "    print(f\"Dropping {mask.sum()} columns with all NaNs before cutoff\")\n",
    "\n",
    "    return arr[:, ~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "def read_and_scale_tables(csv_file_paths,\n",
    "                          start_date=\"1960-01-01\",\n",
    "                          end_date=\"2024-01-01\",\n",
    "                          train_cutoff_str=\"2018-01-01\"):\n",
    "    \"\"\"\n",
    "    Reads each CSV, aligns to a monthly timeline, scales the data,\n",
    "    and returns:\n",
    "    - table_data_dict: dict[table_name] -> np.array of shape (num_months, k_i)\n",
    "    - scalers: dict[table_name] -> a fitted StandardScaler\n",
    "    - monthly_dates: list of monthly date strings\n",
    "    \"\"\"\n",
    "    monthly_dates = month_range(start_date, end_date)\n",
    "    date_to_idx = {d: i for i, d in enumerate(monthly_dates)}\n",
    "    num_months = len(monthly_dates)\n",
    "\n",
    "    table_data_dict = {}\n",
    "    scalers = {}\n",
    "\n",
    "    # We'll locate the index for the cutoff date for training\n",
    "    train_cutoff_idx = date_to_idx[train_cutoff_str]\n",
    "\n",
    "    for file_path in csv_file_paths:\n",
    "        table_name = file_path.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "        print(\"Reading\", table_name)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        feature_cols = [c for c in df.columns if c != \"DATE_PARSED\"]\n",
    "\n",
    "        # Initialize an array with NaNs\n",
    "        array_data = np.full((num_months, len(feature_cols)), np.nan, dtype=np.float32)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            date_str = str(row[\"DATE_PARSED\"])\n",
    "            if date_str in date_to_idx:\n",
    "                idx = date_to_idx[date_str]\n",
    "                array_data[idx] = row[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        array_data = drop_all_nan_columns_before_cutoff(array_data, train_cutoff_idx)\n",
    "\n",
    "        # Fit a scikit-learn StandardScaler on the training portion\n",
    "        scaler = StandardScaler()\n",
    "        train_data = array_data[:train_cutoff_idx]  # up to but not including test start\n",
    "\n",
    "        # In order to handle nulls, we will fill them with the mean of the column before fitting\n",
    "        col_means = np.nanmean(train_data, axis=0)\n",
    "        train_data_copy = train_data.copy()\n",
    "        # Then fill the actual array's NaNs with that column mean\n",
    "        for c in range(train_data.shape[1]):\n",
    "            np.place(train_data_copy[:, c], np.isnan(train_data_copy[:, c]), col_means[c])\n",
    "\n",
    "        scaler.fit(train_data_copy)\n",
    "\n",
    "        # Transform the entire array, ignoring NaNs by temporarily filling them\n",
    "        # then re-inserting them\n",
    "        array_copy = array_data.copy()\n",
    "        nan_mask = np.isnan(array_copy)\n",
    "        array_copy[nan_mask] = 0.0  # placeholder\n",
    "\n",
    "        # scale\n",
    "        scaled_data = scaler.transform(array_copy)\n",
    "        # put the NaNs back\n",
    "        scaled_data[nan_mask] = np.nan\n",
    "\n",
    "        table_data_dict[table_name] = scaled_data\n",
    "        scalers[table_name] = scaler\n",
    "\n",
    "    return table_data_dict, scalers, monthly_dates\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:15.262703800Z",
     "start_time": "2025-01-06T00:21:15.242299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "class EconDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 table_data_dict,\n",
    "                 monthly_dates,\n",
    "                 window_length=60,        # e.g. 5 years if monthly\n",
    "                 train=True,\n",
    "                 test_start_date=\"2018-01-01\",\n",
    "                 p_mask_year=0.2,\n",
    "                 p_mask_partial=0.3,\n",
    "                 p_mask_none=0.5):\n",
    "        \"\"\"\n",
    "        table_data_dict: dict[table_name] -> (num_months, k_i) scaled arrays\n",
    "        monthly_dates: list of str, aligned\n",
    "        window_length: number of months in each sample\n",
    "        train: whether this is train or test\n",
    "        test_start_date: boundary\n",
    "        p_mask_year: probability that we mask the last 12 months entirely\n",
    "        p_mask_partial: probability that we do random partial feature masking\n",
    "        p_mask_none: probability that we do no additional masking\n",
    "        \"\"\"\n",
    "        assert abs((p_mask_year + p_mask_partial + p_mask_none) - 1.0) < 1e-7, \\\n",
    "            \"Mask probabilities must sum to 1.0\"\n",
    "\n",
    "        self.table_data_dict = table_data_dict\n",
    "        self.monthly_dates = monthly_dates\n",
    "        self.num_months = len(monthly_dates)\n",
    "        self.window_length = window_length\n",
    "\n",
    "        self.p_mask_year = p_mask_year\n",
    "        self.p_mask_partial = p_mask_partial\n",
    "        self.p_mask_none = p_mask_none\n",
    "\n",
    "        self.test_start_idx = self.monthly_dates.index(test_start_date)\n",
    "\n",
    "        if train:\n",
    "            # sample windows from [0 .. test_start_idx - window_length]\n",
    "            max_start = self.test_start_idx - window_length\n",
    "            self.start_indices = list(range(0, max_start + 1))\n",
    "        else:\n",
    "            # sample windows from [test_start_idx .. num_months - window_length]\n",
    "            max_start = self.num_months - window_length\n",
    "            self.start_indices = list(range(self.test_start_idx, max_start + 1))\n",
    "\n",
    "        self.table_names = list(self.table_data_dict.keys())\n",
    "        self.table_shapes = [self.table_data_dict[tn].shape[1] for tn in self.table_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.start_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.start_indices[idx]\n",
    "        end_idx = start_idx + self.window_length\n",
    "\n",
    "        sample_full = {}   # unmasked ground truth\n",
    "        sample_masked = {} # input features with potential extra masking\n",
    "\n",
    "        # We'll decide which \"masking mode\" to apply\n",
    "        r = np.random.rand()\n",
    "        if r < self.p_mask_year:\n",
    "            mask_mode = 'year'\n",
    "        elif r < self.p_mask_year + self.p_mask_partial:\n",
    "            mask_mode = 'partial'\n",
    "        else:\n",
    "            mask_mode = 'none'\n",
    "\n",
    "        for tn in self.table_names:\n",
    "            # shape: (L, k_i)\n",
    "            table_slice = self.table_data_dict[tn][start_idx:end_idx]\n",
    "\n",
    "            # Keep a copy for the target\n",
    "            full_data = table_slice.copy()\n",
    "\n",
    "            # This will be our masked input\n",
    "            masked_data = table_slice.copy()\n",
    "\n",
    "            # ~~~ 1) Possibly mask last year (12 months) ~~~\n",
    "            if mask_mode == 'year':\n",
    "                omit_start = max(0, self.window_length - 12)\n",
    "                masked_data[omit_start:, :] = np.nan\n",
    "\n",
    "            # ~~~ 2) Possibly mask partial ~~~\n",
    "            elif mask_mode == 'partial':\n",
    "                # We'll choose random fraction of features to mask across entire L\n",
    "                # For demonstration, let's randomly pick 1/3 of columns to mask\n",
    "                k_i = table_slice.shape[1]\n",
    "                # pick random subset of columns\n",
    "                col_indices = np.random.choice(k_i, size=int(k_i/3), replace=False)\n",
    "                masked_data[:, col_indices] = np.nan\n",
    "\n",
    "            # ~~~ 3) 'none' => do nothing additional ~~~\n",
    "\n",
    "            sample_full[tn] = full_data\n",
    "            sample_masked[tn] = masked_data\n",
    "\n",
    "        return {\n",
    "            \"full_data\": sample_full,\n",
    "            \"masked_data\": sample_masked\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:15.621267200Z",
     "start_time": "2025-01-06T00:21:15.610637300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "def econ_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List of size B\n",
    "      each element is a dict:\n",
    "        {\n",
    "          \"full_data\": {table_name -> (L, k_i) array},\n",
    "          \"masked_data\": {table_name -> (L, k_i) array}\n",
    "        }\n",
    "    Returns a dict:\n",
    "      {\n",
    "        \"full_data\": {table_name -> (B, L, k_i)},\n",
    "        \"masked_data\": {table_name -> (B, L, k_i)},\n",
    "        \"mask\": {table_name -> (B, L, k_i)}\n",
    "      }\n",
    "    \"\"\"\n",
    "    table_names = batch[0][\"full_data\"].keys()\n",
    "\n",
    "    # We'll accumulate data in dictionaries of lists\n",
    "    full_data_dict = {}\n",
    "    masked_data_dict = {}\n",
    "    mask_dict = {}\n",
    "\n",
    "    B = len(batch)\n",
    "\n",
    "    for tn in table_names:\n",
    "        # gather arrays for each sample in batch\n",
    "        full_list = []\n",
    "        masked_list = []\n",
    "        mask_list = []\n",
    "\n",
    "        for sample in batch:\n",
    "            full_np = sample[\"full_data\"][tn]   # shape: (L, k_i)\n",
    "            masked_np = sample[\"masked_data\"][tn] # shape: (L, k_i)\n",
    "\n",
    "            # Convert to torch\n",
    "            full_tensor = torch.tensor(full_np, dtype=torch.float32)\n",
    "            masked_tensor = torch.tensor(masked_np, dtype=torch.float32)\n",
    "\n",
    "            # Build a mask of where full_data is not nan (the ground truth)\n",
    "            valid_mask = ~torch.isnan(full_tensor)  # shape: (L, k_i)\n",
    "\n",
    "            # Replace nans in masked_data with 0.0 for the input\n",
    "            masked_tensor[torch.isnan(masked_tensor)] = 0.0\n",
    "\n",
    "            full_list.append(full_tensor)\n",
    "            masked_list.append(masked_tensor)\n",
    "            mask_list.append(valid_mask.float())  # store as float 0/1\n",
    "\n",
    "        # stack along batch dimension => (B, L, k_i)\n",
    "        full_data_stack = torch.stack(full_list, dim=0)\n",
    "        masked_data_stack = torch.stack(masked_list, dim=0)\n",
    "        mask_stack = torch.stack(mask_list, dim=0)\n",
    "\n",
    "        full_data_dict[tn] = full_data_stack\n",
    "        masked_data_dict[tn] = masked_data_stack\n",
    "        mask_dict[tn] = mask_stack\n",
    "\n",
    "    return {\n",
    "        \"full_data\": full_data_dict,\n",
    "        \"masked_data\": masked_data_dict,\n",
    "        \"mask\": mask_dict\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:15.822043800Z",
     "start_time": "2025-01-06T00:21:15.805816300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "class TableEmbedding(nn.Module):\n",
    "    def __init__(self, k_in, embed_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(k_in, embed_dim)\n",
    "        self.l2 = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, k_in)\n",
    "        returns: (B, L, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:15.941380700Z",
     "start_time": "2025-01-06T00:21:15.930712200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "class Flattened2DTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=32, n_heads=4, dim_feedforward=128, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We'll define a max L, max N for positional embeddings if you want to be strict\n",
    "        # but here we generate them on the fly in forward (less efficient, but simpler).\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, L, N, E)\n",
    "        Flatten -> (B, L*N, E), add pos embeddings, pass through Transformer\n",
    "        -> (B, L, N, E)\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        B, L, N, E = x.shape\n",
    "        S = L*N\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(B, S, E)  # (B, S, E)\n",
    "\n",
    "        # Build positional embeddings\n",
    "        time_positions = torch.arange(L, device=device).unsqueeze(1).expand(L, N).flatten()  # shape (S,)\n",
    "        time_emb = nn.Embedding(L, E).to(device)\n",
    "\n",
    "        x = x + time_emb(time_positions)\n",
    "        x = x.transpose(0, 1)  # -> (S, B, E)\n",
    "\n",
    "        out = self.transformer(x, src_key_padding_mask=src_key_padding_mask)  # (S, B, E)\n",
    "        out = out.transpose(0, 1).view(B, L, N, E)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:16.083272700Z",
     "start_time": "2025-01-06T00:21:16.070042100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "class TableDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, k_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, k_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, embed_dim)\n",
    "        -> (B, L, k_out)\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:16.204496200Z",
     "start_time": "2025-01-06T00:21:16.185704500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "class EconModel(nn.Module):\n",
    "    def __init__(self, table_names, table_shapes,\n",
    "                 embed_dim=32, n_heads=4, ff_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.table_names = table_names\n",
    "        self.N = len(table_names)\n",
    "\n",
    "        self.table_embeds = nn.ModuleDict()\n",
    "        self.table_decoders = nn.ModuleDict()\n",
    "\n",
    "        # Create embeddings/decoders\n",
    "        for tn, k_in in zip(table_names, table_shapes):\n",
    "            self.table_embeds[tn] = TableEmbedding(k_in, embed_dim)\n",
    "            self.table_decoders[tn] = TableDecoder(embed_dim, k_in)\n",
    "\n",
    "        # 2D Transformer core\n",
    "        self.core_transformer = Flattened2DTransformer(\n",
    "            embed_dim=embed_dim,\n",
    "            n_heads=n_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \"\"\"\n",
    "        batch_data:\n",
    "          {\n",
    "            \"full_data\": {tn -> (B, L, k_i)},\n",
    "            \"masked_data\": {tn -> (B, L, k_i)},\n",
    "            \"mask\": {tn -> (B, L, k_i)}\n",
    "          }\n",
    "        We'll use masked_data as input,\n",
    "        but we decode and eventually compute loss vs. full_data.\n",
    "        Returns a dict {tn -> (B, L, k_i)} of predictions.\n",
    "        \"\"\"\n",
    "        B = None\n",
    "        L = None\n",
    "\n",
    "        # 1) embed each table => (B, L, E), stack => (B, L, N, E)\n",
    "        embed_list = []\n",
    "        table_valid_mask_list = []\n",
    "\n",
    "        for tn in self.table_names:\n",
    "            x = batch_data[\"masked_data\"][tn]  # (B, L, k_i), with 0 where missing\n",
    "            m = batch_data[\"mask\"][tn]         # (B, L, k_i)\n",
    "            B, L, k_i = x.shape\n",
    "\n",
    "            # embed\n",
    "            x_emb = self.table_embeds[tn](x)  # -> (B, L, E)\n",
    "            embed_list.append(x_emb)\n",
    "\n",
    "            # Build a \"valid\" mask for the transformer. We can say if the entire row is missing, we mask it.\n",
    "            # But let's do a simple approach: if sum over k_i is zero, it's missing.\n",
    "            # Actually, better to check if masked_data was all zeros => but that's\n",
    "            # tricky if partial columns are present.\n",
    "            # We'll do a simpler approach:\n",
    "            # We'll rely on the standard transformer src_key_padding_mask usage in forward().\n",
    "            # That requires shape (B, L*N). We'll build that after stacking.\n",
    "\n",
    "        # stack => (B, L, N, E)\n",
    "        embed_stack = torch.stack(embed_list, dim=2)\n",
    "\n",
    "        # Build key_padding_mask => shape (B, L*N).\n",
    "        # We consider a position \"padded\" if the input is entirely 0 for that table at that time\n",
    "        # (assuming masked_data sets missing columns to 0).\n",
    "        # Let's check the sum over E BEFORE the linear, or sum over k_i?\n",
    "        # Right now, we have embed_stack: (B, L, N, E)\n",
    "        # Summation in the original space was easier, but let's do it here:\n",
    "        # We'll reconstruct a mask for \"non-empty\" from the input x.\n",
    "\n",
    "        # We'll do a quick pass to find if x was all zeros =>\n",
    "        # but we only have x inside the loop. Let's do it more systematically:\n",
    "\n",
    "        table_zero_mask_list = []\n",
    "        for tn in self.table_names:\n",
    "            x_original = batch_data[\"masked_data\"][tn]  # (B, L, k_i)\n",
    "            zero_mask = (x_original.abs().sum(dim=2) == 0.0)  # shape: (B, L) boolean\n",
    "            table_zero_mask_list.append(zero_mask)\n",
    "\n",
    "        # stack => (B, L, N)\n",
    "        zero_mask_stacked = torch.stack(table_zero_mask_list, dim=2)\n",
    "        # we want shape (B, L*N) for the src_key_padding_mask => True if padded\n",
    "        key_padding_mask = zero_mask_stacked.view(B, -1)  # (B, L*N)\n",
    "\n",
    "        # 2) pass through the transformer\n",
    "        out_2d = self.core_transformer(embed_stack, src_key_padding_mask=key_padding_mask)\n",
    "        # shape: (B, L, N, E)\n",
    "\n",
    "        # 3) decode table by table\n",
    "        decoded = {}\n",
    "        for i, tn in enumerate(self.table_names):\n",
    "            table_repr = out_2d[:, :, i, :]  # (B, L, E)\n",
    "            out = self.table_decoders[tn](table_repr)  # (B, L, k_i)\n",
    "            decoded[tn] = out\n",
    "\n",
    "        return decoded\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:16.319370800Z",
     "start_time": "2025-01-06T00:21:16.304737600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "def masked_mse_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    pred: (B, L, k_i)\n",
    "    target: (B, L, k_i)\n",
    "    mask: (B, L, k_i)  # 1 where ground truth is valid, 0 where no ground truth\n",
    "    Returns average MSE over valid entries.\n",
    "    \"\"\"\n",
    "    diff = (pred - target) ** 2\n",
    "    diff = diff * mask  # zero out missing\n",
    "    valid_count = mask.sum()\n",
    "    if valid_count > 0:\n",
    "        return diff.sum() / valid_count\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=pred.device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:21:16.452626300Z",
     "start_time": "2025-01-06T00:21:16.435553400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train_econ_model(csv_file_paths,\n",
    "                     epochs=5,\n",
    "                     batch_size=8,\n",
    "                     window_length=60,\n",
    "                     embed_dim=32,\n",
    "                     lr=1e-3,\n",
    "                     p_mask_year=0.2,\n",
    "                     p_mask_partial=0.3,\n",
    "                     p_mask_none=0.5):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1) Read & scale data with scikit-learn\n",
    "    2) Create train/test datasets\n",
    "    3) Model + optimizer\n",
    "    4) Training loop with masked MSE\n",
    "    \"\"\"\n",
    "    # 1) read & scale\n",
    "    table_data_dict, scalers, monthly_dates = read_and_scale_tables(\n",
    "        csv_file_paths,\n",
    "        start_date=\"1960-01-01\",\n",
    "        end_date=\"2024-01-01\",\n",
    "        train_cutoff_str=\"2018-01-01\"\n",
    "    )\n",
    "\n",
    "    # 2) create datasets\n",
    "    train_dataset = EconDataset(\n",
    "        table_data_dict,\n",
    "        monthly_dates,\n",
    "        window_length=window_length,\n",
    "        train=True,\n",
    "        test_start_date=\"2018-01-01\",\n",
    "        p_mask_year=p_mask_year,\n",
    "        p_mask_partial=p_mask_partial,\n",
    "        p_mask_none=p_mask_none\n",
    "    )\n",
    "\n",
    "    test_dataset = EconDataset(\n",
    "        table_data_dict,\n",
    "        monthly_dates,\n",
    "        window_length=window_length,\n",
    "        train=False,\n",
    "        test_start_date=\"2018-01-01\",\n",
    "        p_mask_year=p_mask_year,\n",
    "        p_mask_partial=p_mask_partial,\n",
    "        p_mask_none=p_mask_none\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=econ_collate_fn\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=econ_collate_fn\n",
    "    )\n",
    "\n",
    "    # 3) model + optimizer\n",
    "    table_names = list(table_data_dict.keys())\n",
    "    table_shapes = [table_data_dict[tn].shape[1] for tn in table_names]\n",
    "\n",
    "    model = EconModel(\n",
    "        table_names,\n",
    "        table_shapes,\n",
    "        embed_dim=embed_dim,\n",
    "        n_heads=4,\n",
    "        ff_dim=128,\n",
    "        num_layers=2\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4) Training loop\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for b_idx, batch_data in tqdm(enumerate(train_loader), desc=\"Training\"):\n",
    "            # move data to device\n",
    "            for tn in batch_data[\"full_data\"]:\n",
    "                batch_data[\"full_data\"][tn] = batch_data[\"full_data\"][tn].to(device)\n",
    "                batch_data[\"masked_data\"][tn] = batch_data[\"masked_data\"][tn].to(device)\n",
    "                batch_data[\"mask\"][tn] = batch_data[\"mask\"][tn].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)  # dict {tn -> (B, L, k_i)}\n",
    "\n",
    "            # compute loss\n",
    "            loss_val = 0.0\n",
    "            for tn in table_names:\n",
    "                pred = outputs[tn]\n",
    "                tgt = batch_data[\"full_data\"][tn]\n",
    "                msk = batch_data[\"mask\"][tn]\n",
    "                loss_val += masked_mse_loss(pred, tgt, msk)\n",
    "\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss_val.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / (b_idx + 1)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for b_idx, batch_data in tqdm(enumerate(test_loader), desc=\"Testing\"):\n",
    "                for tn in batch_data[\"full_data\"]:\n",
    "                    batch_data[\"full_data\"][tn] = batch_data[\"full_data\"][tn].to(device)\n",
    "                    batch_data[\"masked_data\"][tn] = batch_data[\"masked_data\"][tn].to(device)\n",
    "                    batch_data[\"mask\"][tn] = batch_data[\"mask\"][tn].to(device)\n",
    "\n",
    "                outputs = model(batch_data)\n",
    "                loss_val = 0.0\n",
    "                for tn in table_names:\n",
    "                    pred = outputs[tn]\n",
    "                    tgt = batch_data[\"full_data\"][tn]\n",
    "                    msk = batch_data[\"mask\"][tn]\n",
    "                    loss_val += masked_mse_loss(pred, tgt, msk)\n",
    "\n",
    "                total_test_loss += loss_val.item()\n",
    "\n",
    "            avg_test_loss = total_test_loss / (b_idx + 1)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} - Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:26:11.096583600Z",
     "start_time": "2025-01-06T00:26:11.076645600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading BALANCE-PAIEMENTS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CHOMAGE-TRIM-NATIONAL\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CLIMAT-AFFAIRES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-CONSO-MEN\n",
      "Dropping 1 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-CONSO-SI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-CPEB\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-CSI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-EMPLOI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-ERE\n",
      "Dropping 2 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-FBCF-SI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-PIB\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNA-2020-TEI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNT-2020-CB\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNT-2020-CSI\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNT-2020-OPERATIONS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CNT-2020-PIB-EQB-RF\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading COM-EXT\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading COMPTES-ETAT\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CONSO-MENAGES-2020\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CONSTRUCTION-LOCAUX\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CONSTRUCTION-LOGEMENTS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CREATIONS-ENTREPRISES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading CREATIONS-ENTREPRISES-METHODE-2022\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading DECES-MORTALITE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading DEFAILLANCES-ENTREPRISES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading DEMANDES-EMPLOIS-NATIONALES\n",
      "Dropping 28 columns with all NaNs before cutoff\n",
      "Reading DETTE-NEGOCIABLE-ETAT\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading DETTE-TRIM-APU-2020\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading EMPLOI-BIT-TRIM\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading EMPLOI-SALARIE-TRIM-NATIONAL\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-ACT-IND\n",
      "Dropping 162 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-ART-BAT\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-COM-DET\n",
      "Dropping 7 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-IND-BAT\n",
      "Dropping 12 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-INV-IND\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-MENAGES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-PROMO-IMMO\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-SERV\n",
      "Dropping 412 columns with all NaNs before cutoff\n",
      "Reading ENQ-CONJ-TP\n",
      "Dropping 1 columns with all NaNs before cutoff\n",
      "Reading ERI-ACTIVITE-PARTIELLE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IC-PROD-CONS-2021\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ICA-2021-COMMERCE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ICA-2021-IND-CONS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ICA-2021-SERVICES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading ICT-2020\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading INDICE-TRAITEMENT-FP\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading INDICES_LOYERS\n",
      "Dropping 30 columns with all NaNs before cutoff\n",
      "Reading IP-PROD-CONS-N-HAB-2021\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IPAGRI-BASE-2020\n",
      "Dropping 309 columns with all NaNs before cutoff\n",
      "Reading IPC-PM-2015\n",
      "Dropping 6 columns with all NaNs before cutoff\n",
      "Reading IPCH-2015\n",
      "Dropping 8 columns with all NaNs before cutoff\n",
      "Reading IPEA-2021\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IPGD-2015\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IPI-1990\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IPI-2021\n",
      "Dropping 6 columns with all NaNs before cutoff\n",
      "Reading IPLA-IPLNA-2015\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IPPI-2021\n",
      "Dropping 10 columns with all NaNs before cutoff\n",
      "Reading IPPMP-NF\n",
      "Dropping 2 columns with all NaNs before cutoff\n",
      "Reading IPPS-2021\n",
      "Dropping 3 columns with all NaNs before cutoff\n",
      "Reading IPS-2021-SERVICES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading IRL\n",
      "Dropping 4 columns with all NaNs before cutoff\n",
      "Reading NAISSANCES-FECONDITE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading PARC-LOGEMENTS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading POPULATION-STRUCTURE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading SALAIRES-ACEMO\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading SALAIRES-ACEMO-2017\n",
      "Dropping 1 columns with all NaNs before cutoff\n",
      "Reading SALAIRES-ANNUELS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading SMIC-COTISATIONS\n",
      "Dropping 12 columns with all NaNs before cutoff\n",
      "Reading TAUX-CHOMAGE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-CONDITIONSDEVIE-LOG-SOC\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-CONDITIONSDEVIE-TXP-CDE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-APP-JEN\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-DIPLOMES-TECHNIQUES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-EFF-PSS\n",
      "Dropping 351 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-ENS-PSD\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-ETAB-SCOL\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EDUCATION-REU-BAC\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EMPLOI-31-DECEMBRE-BASE-2018\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-EMPLOI-SALARIE-TRIM\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-ENTREPRISES-EFF-OPE\n",
      "Dropping 142 columns with all NaNs before cutoff\n",
      "Reading TCRED-ENTREPRISES-EMP-SAL-AN-TAILLE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-ESTIMATIONS-POPULATION\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-SALAIRES-REVENUS-MEN\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-SALAIRES-REVENUS-RET-BEN\n",
      "Dropping 8 columns with all NaNs before cutoff\n",
      "Reading TCRED-SALAIRES-REVENUS-REV-SAL-SEXE-CS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-SALAIRES-REVENUS-TAUX-PAUVRETE-AGE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-SALAIRES-REVENUS-TNB\n",
      "Dropping 229 columns with all NaNs before cutoff\n",
      "Reading TCRED-SANTE-ACCUEIL-PERS-AGEES\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-SANTE-PERSONNELS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-TRAVAIL-EMPLOI-EFF-FP-RATIO\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-TRAVAIL-EMPLOI-EFF-FPE\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-TRAVAIL-EMPLOI-EFF-FPH\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-TRAVAIL-EMPLOI-EFF-FPT\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TCRED-TRAVAIL-EMPLOI-TCHOMA-SA\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading TOURISME-FRANCE-METHODE-REDRESSEMENT-2019\n",
      "Dropping 512 columns with all NaNs before cutoff\n",
      "Reading TRANSPORTS\n",
      "Dropping 0 columns with all NaNs before cutoff\n",
      "Reading VOV-2021-COMMERCE\n",
      "Dropping 0 columns with all NaNs before cutoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\anaconda3\\envs\\Oracle\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed92b90460594bd4b4852e95772b7396"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[158], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m     all_data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m      6\u001B[0m csv_file_paths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m table_name \u001B[38;5;129;01min\u001B[39;00m all_data]\n\u001B[1;32m----> 7\u001B[0m \u001B[43mtrain_econ_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_file_paths\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[157], line 104\u001B[0m, in \u001B[0;36mtrain_econ_model\u001B[1;34m(csv_file_paths, epochs, batch_size, window_length, embed_dim, lr, p_mask_year, p_mask_partial, p_mask_none)\u001B[0m\n\u001B[0;32m    101\u001B[0m     msk \u001B[38;5;241m=\u001B[39m batch_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmask\u001B[39m\u001B[38;5;124m\"\u001B[39m][tn]\n\u001B[0;32m    102\u001B[0m     loss_val \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m masked_mse_loss(pred, tgt, msk)\n\u001B[1;32m--> 104\u001B[0m \u001B[43mloss_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    105\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    107\u001B[0m total_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_val\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Oracle\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Oracle\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Oracle\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Data/all_data.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "csv_file_paths = [f\"Data/{table_name}.csv\" for table_name in all_data]\n",
    "train_econ_model(csv_file_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T00:28:41.913282600Z",
     "start_time": "2025-01-06T00:26:17.226634600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
